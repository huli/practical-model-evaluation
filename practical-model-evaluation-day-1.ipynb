{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "These notebooks are part of Kaggle‚Äôs [Practical Model Evaluation](https://www.kaggle.com/practical-model-evaluation) event, which ran from December 3-5 2019. You can find [the livestreams for this event here](https://youtu.be/7RdKnACscjA?list=PLqFaTIg4myu-HA1VGJi_7IGFkKRoZeOFt).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "During a [Kaggle competition](https://www.kaggle.com/competitions), we evaluate your models in a specific way: the predictions you make are compared to a ground truth using a specific metric. (Which metric depends on the competition and the question we‚Äôre asking.) Whichever model achieves the highest score on the final validation dataset wins. Pretty simple right?\n",
    "\n",
    "If you‚Äôre working on building machine learning models in a work setting, however, things may be a bit more complicated. Achieving a good score on your metric of choice is important, of course, but it‚Äôs only part of the problem. When picking the best model to use for a particular problem, some of the things you need to consider include: \n",
    "\n",
    "* Your time\n",
    "* Computation time and cost for training and inference\n",
    "* Model performance (and not just your loss function!)\n",
    "\n",
    "Let‚Äôs talk about each of these points in turn and then how to balance them when picking what type of model to work on.\n",
    "\n",
    "## Your Time\n",
    "\n",
    "It‚Äôs easy to forget that **your time is a limited and valuable resource**. It can be hard to predict how long a data science project will take, but here are some things that can take more time than you might anticipate.\n",
    "\n",
    "* **Scoping projects.** What‚Äôs feasible? What do you currently have enough data to be able to do? What would be the most valuable type of model? What timelines are you working with? Figuring out the answers to these questions, and making sure that your stakeholders agree, can be extremely time consuming. (If you're lucky enough to be working with a project or product manager they should be able to help you here.)\n",
    "* **Setting up your environment and installing dependencies.** True story: one year in grad school I ended up spending an entire summer wrestling with different audio codecs and their dependencies! It can be easy to underestimate how much time it will take to get a new modelling framework and all its dependencies set up. Especially for very new frameworks, you may end up finding brand new problems or bugs that you‚Äôll need to solve before you can even start training your model.\n",
    "* **Data collection and preparation.** If you‚Äôre lucky, the data you need will already exist. If you‚Äôre *really* lucky it will already be in a form that you can use for modelling. Probably at least one of these things will not be true and correcting it will inevitably end up taking far, far longer than you think it should. According to the [2018 Kaggle machine learning and data science survey](https://www.kaggle.com/kaggle/kaggle-survey-2018), data scientists spend over half of their time gathering, cleaning and visualizing data.\n",
    "* **Communicating with stakeholders.** If you build a model that no one ever uses, what‚Äôs the point? ü§∑ Working with stakeholders (i.e. people who have an interest in your work) is really, really important to make sure that 1) you‚Äôre building a model that addresses a real need, 2) your stakeholders understand the strengths and limitations of your model and also machine learning in general and 3) your model actually gets used.\n",
    "\n",
    "With these things in mind, here are some tips you can use to reduce the amount of time it takes you to get your model up and training :\n",
    "\n",
    "* It‚Äôs often a good idea to start with an established, older model family and implementation. There will likely be more example code for you to work from and, hopefully, fewer bugs. This is especially true if you already have it installed correctly to train a model with whatever compute you‚Äôre using. \n",
    "* If you can, try to find a container (like a Docker) that has all the dependencies with the correct versions already set up for the packages you want to use. This will help you save on set up time.\n",
    "* For data cleaning, before you start working spend some time writing out what you need your data to look like before it goes in the model. Then list out all the steps you need to go through to get your data to that form. This will keep you from too sidetracked during data cleaning and give you a nice checklist to work through and track your progress on.\n",
    "* If your data is tabular and in a SQL database, do as much of your data cleaning as possible in SQL. A well written SQL query is generally *much* faster than running the equivalent data manipulations in Python or R.\n",
    "\n",
    "## Compute time\n",
    "\n",
    "If you‚Äôve trained larger models before, this a limitation you‚Äôre very familiar with. The more trainable parameters a model has and the more data you‚Äôre using to train it, the longer it will take. And, since computing time costs money (either in electricity if you‚Äôre working on your own machines or being billed for the time if you‚Äôre using somebody else‚Äôs). The initial training time isn‚Äôt the only factor to consider, however. Here are some other things you might not have considered:\n",
    "\n",
    "* **How long will it take to update your model?** Depending on your specific project, you may need to regularly update your model or retrain a new one from scratch. Some models, like neural networks, can generally be updated. For other models, especially random forests, it‚Äôs often easiest to retrain from scratch again. If you‚Äôll need to retrain your model often you might consider\n",
    "* **How long does it take your model to make predictions?** This is often called ‚Äúinference time‚Äù and it‚Äôs really easy to forget to check in the excitement of training models! If you‚Äôre model is very accurate but so slow that everyone who tries to use it quits before they get their results back it‚Äôs probably not actually a very good model. \n",
    "\n",
    "Depending on your specific problem, **you‚Äôll have to choose how you want to balance the time it takes to train your model, update your trained model and use your model to make predictions**.\n",
    "\n",
    "## Model performance\n",
    "\n",
    "The most common way to measure a given model is it‚Äôs loss metric. (I personally generally go with cross entropy for classification and, as long as I care about outliers, mean squared error for regression.) However, while these metrics are very useful for training they can pretty easily hide important differences between individual models. \n",
    "\n",
    "### Error analysis\n",
    "\n",
    "This is where *error analysis* comes into play. In machine learning, people generally use ‚Äúerror analysis‚Äù to mean looking at how many and what sorts of errors a model made. This can be helpful during model training and tuning to help you identify places where your model can be improved, usually through additional feature engineering or data preprocessing. \n",
    "\n",
    "Including a discussion of error analysis with your final model can also help build trust. \n",
    "\n",
    "> All machine learning models make errors. It‚Äôs important to be able to clearly communicate the types of errors your model is likely to make and consider that when selecting which model to implement.\n",
    "\n",
    "For this workshop, we‚Äôll be looking at multi-class classification problems and using confusion matrices to quickly summarize errors. \n",
    "\n",
    "### Interpretability\n",
    "\n",
    "Another important thing to consider when evaluating model performance is *interpretability*. Specifically, *why* did a given model output a specific decision for a specific class? When you‚Äôre working with stakeholders who have a lot of knowledge about the data you‚Äôre working with, being able to offer an answer to this question can help build trust in your model. \n",
    "\n",
    "For this particular workshop, we‚Äôll be using counterfactuals as a way to interpret model decisions. Counterfactuals let you ask ‚Äúwhat feature would I need to change and in what way in order to get a different output?‚Äù or, in the easier-to-compute case, ‚Äúhow would my model output change if I changed a single feature in a specific way?‚Äù. \n",
    "\n",
    "> Counterfactuals have two main advantages: you can use them for any class of model and it‚Äôs easy for someone without much of a machine learning background to understand. Which is important; not everyone on your team is going to have--or need--a deep background in machine learning.\n",
    "\n",
    "We‚Äôll talk more about them later on, but for now if you‚Äôre curious you can check out [this chapter in Christoph Molnar‚Äôs book ‚ÄúInterpretable Machine Learning\n",
    "A Guide for Making Black Box Models Explainable‚Äù for more details](https://christophm.github.io/interpretable-ml-book/counterfactual.html#generating-counterfactual-explanations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Exercise!\n",
    "\n",
    "For the purposes of this workshop, we‚Äôll be working on a project to predict what job title a data-science-related role will have given the responsibilities of the role using data from the 2018 & 2019 Kaggle Data Science Survey.\n",
    "\n",
    "Let‚Äôs pretend that you‚Äôre working for a consulting company that helps other companies hire data scientists. We‚Äôll be building this tool as an automated first step for people interested in consulting with the company to narrow down what type of role they‚Äôre looking for, and that information will be used to help match them with the most relevant consultant on the team.\n",
    "\n",
    "With this in mind, take some time to think about and answer the following questions. If you‚Äôd like to share, you can post your answers in the comments below.\n",
    "\n",
    "* Take a look [at the 2019 survey data](https://www.kaggle.com/c/kaggle-survey-2019). Which particular fields might make good features? How much cleaning will this data need for you to be ready to train a classification model on it? (I'll be providing one version of the cleaned data, but if you have time you cad do your own data cleaning and feature engineering.)\n",
    "* Data science is a fast moving field and job titles change quickly. On the other hand, hiring events are relatively rare. Given how often you‚Äôll need to retrain your model and also that it isn't going to need to be run very often, how would you balance training time, retraining time and inference time?\n",
    "* What sorts of errors should you (or your imaginary stakeholders) be particularly worried about given the subject matter of the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
